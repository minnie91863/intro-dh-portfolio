{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chronicling America API\n",
    "\n",
    "[Chronicling America](https://chroniclingamerica.loc.gov/) is a collection of digitized American newspapers dating from 1777 to 1963 provided by the Library of Congress. The collection offers an application programming interface (api) which allows users to easily harvest large amounts of data.\n",
    "\n",
    "In this notebook we will search Chronicling America's api, gather the search results into a Pandas dataframe, clean the data, and save it as a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import requests\n",
    "import json\n",
    "import math\n",
    "import pandas as pd\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chronicling America URLs\n",
    "\n",
    "If I search for a term, \"socialism\" for example, on https://chroniclingamerica.loc.gov/ I will get a results url that looks like this:\n",
    "\n",
    "https://chroniclingamerica.loc.gov/search/pages/results/?state=&date1=1777&date2=1963&proxtext=socialism&x=16&y=8&dateFilterType=yearRange&rows=20&searchType=basic\n",
    "\n",
    "These search results are human actionable, but not machine actionable. Chronicling America as an API that allows me to get machine actionable results if I add `&format=json`:\n",
    "\n",
    "https://chroniclingamerica.loc.gov/search/pages/results/?state=&date1=1777&date2=1963&proxtext=socialism&x=16&y=8&dateFilterType=yearRange&rows=20&searchType=basic&format=json\n",
    "\n",
    "If we examine the url we see that there are a number of search parameters:\n",
    "* `state=`\n",
    "* `date1=1777`\n",
    "* `date2=1963`\n",
    "* `proxtext=socialism`\n",
    "\n",
    "We can edit these values to modify our search. Here I will limit my date range from 1945-1963:\n",
    "\n",
    "https://chroniclingamerica.loc.gov/search/pages/results/?state=&date1=1945&date2=1963&proxtext=socialism&x=16&y=8&dateFilterType=yearRange&rows=20&searchType=basic&format=json\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I can use the `requests` library to retrieve data from the url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial search\n",
    "url = 'https://chroniclingamerica.loc.gov/search/pages/results/?state=&date1=1945&date2=1963&proxtext=socialism&x=16&y=8&dateFilterType=yearRange&rows=20&searchType=basic&format=json'\n",
    "response = requests.get(url)  # returns a 'object' representing the webpage\n",
    "raw = response.text  # the text attribute contains the text from the web page as a string\n",
    "results = json.loads(raw)  # the loads method from the json library transforms the string into a dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the resutls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('totalItems:', results['totalItems'])\n",
    "print('endIndex:', results['endIndex'])\n",
    "print('startIndex:', results['startIndex'])\n",
    "print('itemsPerPage:', results['itemsPerPage'])\n",
    "print('Length and type of items:', len(results['items']), type(results['items']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Chronicling America API returned 209,609 results. However, it will only display 20 at a time by default. I can add a new parameter `page=` to cycle through all the results, but first I need to know how many pages there will be. I can find this out by dividing `totalItems` (209,609) by `itemsPerPage` (20) and then round-up using `math.ceil`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find total amount of pages\n",
    "total_pages = math.ceil(results['totalItems'] / results['itemsPerPage'])\n",
    "print(total_pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I know how many pages there will be, I can use a for loop to iterate through each result page and then each item on each result page. I then gather the data I want from each item: newspaper title, city, date, and text. \n",
    "\n",
    "Notice in the code below I placed the url string in parentheses () so that I could break it up over multiple lines making it easier to read.\n",
    "\n",
    "Also, for the sake of this demonstration, I am only iterating over 10 pages. For the full results the for loop should begin: `for i in range(1, total_pages+1)` (the `+1` is necessary becase the seond number in the range function is exclusive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query the api and save to dict \n",
    "data = []\n",
    "start_date = '1945'\n",
    "end_date = '1963'\n",
    "search_term = 'socialism'\n",
    "for i in range(1, 11):  # for sake of time I'm doing only 10, you will want to put total_pages+1\n",
    "    url = (f'https://chroniclingamerica.loc.gov/search/pages/results/?state=&date1={start_date}'\n",
    "           f'&date2={end_date}&proxtext={search_term}&x=16&y=8&dateFilterType=yearRange&rows=20'\n",
    "           f'&searchType=basic&format=json&page={i}')  # f-string\n",
    "    response = requests.get(url)\n",
    "    raw = response.text\n",
    "    print(response.status_code)  # checking for errors\n",
    "    results = json.loads(raw)\n",
    "    items_ = results['items']\n",
    "    for item_ in items_:\n",
    "        temp_dict = {}\n",
    "        temp_dict['title'] = item_['title_normal']\n",
    "        temp_dict['city'] = item_['city']\n",
    "        temp_dict['date'] = item_['date']\n",
    "        temp_dict['raw_text'] = item_['ocr_eng']\n",
    "        data.append(temp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a backup of the results in case we get timed out in class\n",
    "#with open('../data/backup-data.json', 'w') as f:\n",
    "#    json.dump(data, f)\n",
    "\n",
    "#with open('../data/backup-data.json', 'r') as f:\n",
    "#    data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert dictionary to a Pandas dataframe\n",
    "Now that we have the information we want in a dictionary called `data`, we can put it into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert dict to dataframe\n",
    "df = pd.DataFrame.from_dict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change date format\n",
    "Pandas allows us to clean and edit our data easily (relatively). We can first convert the string values in the date column to properly formated dates and then sort the dataframe by date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert date column from string to date-time object\n",
    "df['date'] = pd.to_datetime(df['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort by date\n",
    "sorted_df = df.sort_values(by='date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process text\n",
    "We can now porcess our text for analysis. The text provded by Chronicling America comes from optical character recognition (ocr) and the accuracy of ocr can be low. Here I will remove new line characters (`\\n`), stop words, and then lemamtize the text.\n",
    "\n",
    "**Rememeber** the decisions you make in how to process your text should be based on the kind of analysis you want to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write fuction to process text\n",
    "# load nlp model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.disable_pipes('ner', 'parser')  # these are unnecessary for the task at hand\n",
    "\n",
    "junk_words = ['hesse']  # you can add any words you want removed here\n",
    "\n",
    "def process_text(text):\n",
    "    \"\"\"Remove new line characters and lemmatize text. Returns string of lemmas\"\"\"\n",
    "    text = text.replace('\\n', ' ')\n",
    "    doc = nlp(text)\n",
    "    tokens = [token for token in doc]\n",
    "    no_stops = [token for token in tokens if not token.is_stop]\n",
    "    no_punct = [token for token in no_stops if token.is_alpha]\n",
    "    lemmas = [token.lemma_ for token in no_punct]\n",
    "    lemmas_lower = [lemma.lower() for lemma in lemmas]\n",
    "    lemmas_string = ' '.join(lemmas_lower)\n",
    "    return lemmas_string\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# apply process_text function\n",
    "sorted_df['lemmas'] = sorted_df['raw_text'].apply(process_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sorted_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save as a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to csv\n",
    "sorted_df.to_csv(f'../data/{search_term}{start_date}-{end_date}.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
