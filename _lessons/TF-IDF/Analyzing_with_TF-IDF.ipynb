{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d71fa713",
   "metadata": {},
   "source": [
    "# Analyzing Documents with TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59413e6a",
   "metadata": {},
   "source": [
    "## Source"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4910ca8",
   "metadata": {},
   "source": [
    "[https://programminghistorian.org/en/lessons/analyzing-documents-with-tfidf#tf-idf-and-common-alternatives](https://programminghistorian.org/en/lessons/analyzing-documents-with-tfidf#tf-idf-and-common-alternatives)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5cbd680",
   "metadata": {},
   "source": [
    "## Reflection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4457a7",
   "metadata": {},
   "source": [
    "In this Programming Historian Lesson, the basic functionality of Term Frequency - Inverse Document Frequency (tf-idf) is introduced through a short example in Python, some variations of how it can be utilized, and some other NLP alternatives to using tf-idf. I learned that, unlike other methods of nlp frequency analysis, tf-idf is largely algorithmic in how it determines similarities and trends within a text corpus. There are many different methods of implementing tf-idf. One implementation called Scikit-Learn’s idf transformation mathematically defines the inverse document frequency (idf) of a text as\n",
    "\n",
    "idf_i = ln[(N+1) / df_i] + 1\n",
    "\n",
    "and therefore the total tf-idf  as\n",
    "\n",
    "tf-idf_i = tf_i × idf_i\n",
    "\n",
    "Since this process is largely quantitative, discrete parameters can be applied to this process, such as defining stopwords, minimum and maximum values, and certain min/max creatures and ranges. Thus, the resulting data can be more useful in other applications, such as machine learning and algorithmic searches. \n",
    "\n",
    "Compared to the other Programming Historian Lessons I completed, this one included minimal, but dense, coding that walked through the process of storing, sorting, and converting the text from a .txt file to a workable .csv file. The only issues I had with the tutorial was that one function, get_feature_names(), gave me a warning with the type, so I changed it to get_feature_names_out(). I also ran into an issue where instead of outputting the converted .csv files to the new directory we made in the lesson, they instead were outputted into the same file. I couldn’t figure out what was causing this issue, since the code seemed to be changing the file output directory.\n",
    "\n",
    "Otherwise, the lesson stepping through the coding process was well-explained and highly technical. The reading in the second half was informative and served as a good introduction to the many ways data can be organized to be analyzed, and great examples of some issues that may arise based on the dataset’s qualities. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80b3b5a",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd33020",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "all_txt_files =[]\n",
    "for file in Path(\"txt\").rglob(\"*.txt\"):\n",
    "     all_txt_files.append(file.parent / file.name)\n",
    "# counts the length of the list\n",
    "n_files = len(all_txt_files)\n",
    "print(n_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3124990f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_txt_files.sort()\n",
    "all_txt_files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d755035",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_docs = []\n",
    "for txt_file in all_txt_files:\n",
    "    with open(txt_file) as f:\n",
    "        txt_file_as_string = f.read()\n",
    "    all_docs.append(txt_file_as_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03472adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the TfidfVectorizer from Scikit-Learn.  \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    " \n",
    "vectorizer = TfidfVectorizer(max_df=.65, min_df=1, stop_words=None, use_idf=True, norm=None)\n",
    "transformed_documents = vectorizer.fit_transform(all_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4883d235",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_documents_as_array = transformed_documents.toarray()\n",
    "# use this line of code to verify that the numpy array represents the same number of documents that we have in the file list\n",
    "len(transformed_documents_as_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfd2307",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# make the output folder if it doesn't already exist\n",
    "Path(\"./tf_idf_output\").mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f3a8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct a list of output file paths using the previous list of text files the relative path for tf_idf_output\n",
    "output_filenames = [(str(txt_file).replace(\".txt\", \".csv\")).replace(\"txt/\", \"tf_idf_output/\") for txt_file in all_txt_files]\n",
    "\n",
    "# loop each item in transformed_documents_as_array, using enumerate to keep track of the current position\n",
    "for counter, doc in enumerate(transformed_documents_as_array):\n",
    "    # construct a dataframe\n",
    "    tf_idf_tuples = list(zip(vectorizer.get_feature_names_out(), doc))\n",
    "    one_doc_as_df = pd.DataFrame.from_records(tf_idf_tuples, columns=['term', 'score']).sort_values(by='score', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    # output to a csv using the enumerated value for the filename\n",
    "    one_doc_as_df.to_csv(output_filenames[counter])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
