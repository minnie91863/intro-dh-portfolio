{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hApv2hxy_KO0"
   },
   "source": [
    "# Exploring Gibbon - Process Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.4.3-cp39-cp39-win_amd64.whl (11.9 MB)\n",
      "     ---------------------------------------- 11.9/11.9 MB 8.5 MB/s eta 0:00:00\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.10\n",
      "  Downloading spacy_legacy-3.0.10-py2.py3-none-any.whl (21 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.8-cp39-cp39-win_amd64.whl (96 kB)\n",
      "     ---------------------------------------- 96.8/96.8 kB 5.4 MB/s eta 0:00:00\n",
      "Collecting typer<0.8.0,>=0.3.0\n",
      "  Downloading typer-0.7.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\minni\\anaconda3\\lib\\site-packages (from spacy) (21.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\minni\\anaconda3\\lib\\site-packages (from spacy) (2.28.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\minni\\anaconda3\\lib\\site-packages (from spacy) (1.21.5)\n",
      "Collecting catalogue<2.1.0,>=2.0.6\n",
      "  Downloading catalogue-2.0.8-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\minni\\anaconda3\\lib\\site-packages (from spacy) (63.4.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\minni\\anaconda3\\lib\\site-packages (from spacy) (4.64.1)\n",
      "Collecting srsly<3.0.0,>=2.4.3\n",
      "  Downloading srsly-2.4.5-cp39-cp39-win_amd64.whl (481 kB)\n",
      "     ------------------------------------- 481.4/481.4 kB 10.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: jinja2 in c:\\users\\minni\\anaconda3\\lib\\site-packages (from spacy) (2.11.3)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.9-cp39-cp39-win_amd64.whl (18 kB)\n",
      "Collecting thinc<8.2.0,>=8.1.0\n",
      "  Downloading thinc-8.1.5-cp39-cp39-win_amd64.whl (1.3 MB)\n",
      "     ---------------------------------------- 1.3/1.3 MB 10.1 MB/s eta 0:00:00\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Downloading spacy_loggers-1.0.3-py3-none-any.whl (9.3 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.7-cp39-cp39-win_amd64.whl (30 kB)\n",
      "Collecting pathy>=0.3.5\n",
      "  Downloading pathy-0.10.0-py3-none-any.whl (48 kB)\n",
      "     ---------------------------------------- 48.9/48.9 kB ? eta 0:00:00\n",
      "Collecting langcodes<4.0.0,>=3.2.0\n",
      "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "     ------------------------------------- 181.6/181.6 kB 10.7 MB/s eta 0:00:00\n",
      "Collecting pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4\n",
      "  Downloading pydantic-1.10.2-cp39-cp39-win_amd64.whl (2.1 MB)\n",
      "     ---------------------------------------- 2.1/2.1 MB 11.3 MB/s eta 0:00:00\n",
      "Collecting wasabi<1.1.0,>=0.9.1\n",
      "  Downloading wasabi-0.10.1-py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\minni\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy) (3.0.9)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in c:\\users\\minni\\anaconda3\\lib\\site-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in c:\\users\\minni\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.3.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\minni\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\minni\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.9.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\minni\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\minni\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3)\n",
      "Collecting confection<1.0.0,>=0.0.1\n",
      "  Downloading confection-0.0.3-py3-none-any.whl (32 kB)\n",
      "Collecting blis<0.8.0,>=0.7.8\n",
      "  Downloading blis-0.7.9-cp39-cp39-win_amd64.whl (7.0 MB)\n",
      "     ---------------------------------------- 7.0/7.0 MB 9.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: colorama in c:\\users\\minni\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.5)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\minni\\anaconda3\\lib\\site-packages (from typer<0.8.0,>=0.3.0->spacy) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\minni\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.0.1)\n",
      "Installing collected packages: wasabi, cymem, spacy-loggers, spacy-legacy, pydantic, murmurhash, langcodes, catalogue, blis, typer, srsly, preshed, pathy, confection, thinc, spacy\n",
      "Successfully installed blis-0.7.9 catalogue-2.0.8 confection-0.0.3 cymem-2.0.7 langcodes-3.3.0 murmurhash-1.0.9 pathy-0.10.0 preshed-3.0.8 pydantic-1.10.2 spacy-3.4.3 spacy-legacy-3.0.10 spacy-loggers-1.0.3 srsly-2.4.5 thinc-8.1.5 typer-0.7.0 wasabi-0.10.1\n"
     ]
    }
   ],
   "source": [
    "!pip install -U spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Z4bCFyUaJ1cA",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import json\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary: Working with files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open a file and write to it\n",
    "text = \"In the second century of the Christian era, the empire of Rome comprehended the fairest part of the earth, and the most civilised portion of mankind. The frontiers of that extensive monarchy were guarded by ancient renown and disciplined valour. The gentle, but powerful, influence of laws and manners had gradually cemented the union of the provinces.\"\n",
    "file_name = 'new_file.txt'\n",
    "with open(file=file_name, encoding='utf-8', mode='w') as f:\n",
    "    f.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the second century of the Christian era, the empire of Rome comprehended the fairest part of the earth, and the most civilised portion of mankind. The frontiers of that extensive monarchy were guarded by ancient renown and disciplined valour. The gentle, but powerful, influence of laws and manners had gradually cemented the union of the provinces.\n"
     ]
    }
   ],
   "source": [
    "# open a file to read from it\n",
    "with open(file=file_name, encoding='utf-8', mode ='r') as f:\n",
    "    opened_text = f.read()\n",
    "print(opened_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary: List Comprehension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['second', 'century', 'Christian', 'era,', 'empire', 'Rome', 'comprehended', 'fairest', 'part', 'earth,', 'most', 'civilised', 'portion', 'mankind.', 'frontiers', 'extensive', 'monarchy', 'were', 'guarded', 'ancient', 'renown', 'disciplined', 'valour.', 'gentle,', 'powerful,', 'influence', 'laws', 'manners', 'had', 'gradually', 'cemented', 'union', 'provinces.']\n"
     ]
    }
   ],
   "source": [
    "# Remember our first function for NLP?\n",
    "text = \"In the second century of the Christian era, the empire of Rome comprehended the fairest part of the earth, and the most civilised portion of mankind. The frontiers of that extensive monarchy were guarded by ancient renown and disciplined valour. The gentle, but powerful, influence of laws and manners had gradually cemented the union of the provinces.\"\n",
    "# Convert the string into a list\n",
    "tokens = text.split(\" \")\n",
    "# Identify stop-words\n",
    "stop_words = [\"in\", \"the\", \"of\", \"and\", \"that\", \"by\", \"but\"]\n",
    "# Create an empty list for tokens that aren't stop words\n",
    "tokens_no_stops = []\n",
    "# Iterate through tokens\n",
    "for token in tokens:\n",
    "    if token.lower() not in stop_words:\n",
    "        tokens_no_stops.append(token)\n",
    "print(tokens_no_stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['second', 'century', 'Christian', 'era,', 'empire', 'Rome', 'comprehended', 'fairest', 'part', 'earth,', 'most', 'civilised', 'portion', 'mankind.', 'frontiers', 'extensive', 'monarchy', 'were', 'guarded', 'ancient', 'renown', 'disciplined', 'valour.', 'gentle,', 'powerful,', 'influence', 'laws', 'manners', 'had', 'gradually', 'cemented', 'union', 'provinces.']\n"
     ]
    }
   ],
   "source": [
    "# The for loop can be re-written as a \"list comprehension\"\n",
    "text_2 = \"In the second century of the Christian era, the empire of Rome comprehended the fairest part of the earth, and the most civilised portion of mankind. The frontiers of that extensive monarchy were guarded by ancient renown and disciplined valour. The gentle, but powerful, influence of laws and manners had gradually cemented the union of the provinces.\"\n",
    "# Convert the string into a list\n",
    "tokens_2 = text_2.split(\" \")\n",
    "# Identify stop-words\n",
    "stop_words = [\"in\", \"the\", \"of\", \"and\", \"that\", \"by\", \"but\"]\n",
    "# List comprehension\n",
    "tokens_no_stops_2 = [token for token in tokens if not token.lower() in stop_words]\n",
    "print(tokens_no_stops_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary: os.listdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gibbon_decline_volume1_chap01.txt\n",
      "gibbon_decline_volume1_chap02.txt\n",
      "gibbon_decline_volume1_chap03.txt\n",
      "gibbon_decline_volume1_chap04.txt\n",
      "gibbon_decline_volume1_chap05.txt\n",
      "gibbon_decline_volume1_chap06.txt\n",
      "gibbon_decline_volume1_chap07.txt\n",
      "gibbon_decline_volume1_chap08.txt\n",
      "gibbon_decline_volume1_chap09.txt\n",
      "gibbon_decline_volume1_chap10.txt\n",
      "gibbon_decline_volume1_chap11.txt\n",
      "gibbon_decline_volume1_chap12.txt\n",
      "gibbon_decline_volume1_chap13.txt\n",
      "gibbon_decline_volume1_chap14.txt\n",
      "gibbon_decline_volume1_chap15.txt\n",
      "gibbon_decline_volume1_chap16.txt\n",
      "gibbon_decline_volume1_chap17.txt\n",
      "gibbon_decline_volume1_chap18.txt\n",
      "gibbon_decline_volume1_chap19.txt\n",
      "gibbon_decline_volume1_chap20.txt\n",
      "gibbon_decline_volume1_chap21.txt\n",
      "gibbon_decline_volume1_chap22.txt\n",
      "gibbon_decline_volume1_chap23.txt\n",
      "gibbon_decline_volume1_chap24.txt\n",
      "gibbon_decline_volume1_chap25.txt\n",
      "gibbon_decline_volume1_chap26.txt\n",
      "gibbon_decline_volume1_chap27.txt\n",
      "gibbon_decline_volume1_chap28.txt\n",
      "gibbon_decline_volume1_chap29.txt\n",
      "gibbon_decline_volume1_chap30.txt\n",
      "gibbon_decline_volume1_chap31.txt\n",
      "gibbon_decline_volume1_chap32.txt\n",
      "gibbon_decline_volume1_chap33.txt\n",
      "gibbon_decline_volume1_chap34.txt\n",
      "gibbon_decline_volume1_chap35.txt\n",
      "gibbon_decline_volume1_chap36.txt\n",
      "gibbon_decline_volume1_chap37.txt\n",
      "gibbon_decline_volume1_chap38.txt\n",
      "gibbon_decline_volume2_chap39.txt\n",
      "gibbon_decline_volume2_chap40.txt\n",
      "gibbon_decline_volume2_chap41.txt\n",
      "gibbon_decline_volume2_chap42.txt\n",
      "gibbon_decline_volume2_chap43.txt\n",
      "gibbon_decline_volume2_chap44.txt\n",
      "gibbon_decline_volume2_chap45.txt\n",
      "gibbon_decline_volume2_chap46.txt\n",
      "gibbon_decline_volume2_chap47.txt\n",
      "gibbon_decline_volume2_chap48.txt\n",
      "gibbon_decline_volume2_chap49.txt\n",
      "gibbon_decline_volume2_chap50.txt\n",
      "gibbon_decline_volume2_chap51.txt\n",
      "gibbon_decline_volume2_chap52.txt\n",
      "gibbon_decline_volume2_chap53.txt\n",
      "gibbon_decline_volume2_chap54.txt\n",
      "gibbon_decline_volume2_chap55.txt\n",
      "gibbon_decline_volume2_chap56.txt\n",
      "gibbon_decline_volume2_chap57.txt\n",
      "gibbon_decline_volume2_chap58.txt\n",
      "gibbon_decline_volume2_chap59.txt\n",
      "gibbon_decline_volume2_chap60.txt\n",
      "gibbon_decline_volume2_chap61.txt\n",
      "gibbon_decline_volume2_chap62.txt\n",
      "gibbon_decline_volume2_chap63.txt\n",
      "gibbon_decline_volume2_chap64.txt\n",
      "gibbon_decline_volume2_chap65.txt\n",
      "gibbon_decline_volume2_chap66.txt\n",
      "gibbon_decline_volume2_chap67.txt\n",
      "gibbon_decline_volume2_chap68.txt\n",
      "gibbon_decline_volume2_chap69.txt\n",
      "gibbon_decline_volume2_chap70.txt\n",
      "gibbon_decline_volume2_chap71.txt\n"
     ]
    }
   ],
   "source": [
    "# use os.listdir to list file names in a directory\n",
    "text_path = \"../text/gibbon_decline_and_fall/\"\n",
    "for file_name in os.listdir(text_path):\n",
    "    print(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KG63QU6DIhgZ"
   },
   "source": [
    "## Process text for analysis using spaCy\n",
    "\n",
    "Before doing NLP work, most texts will need to be preprocessed in different ways. You may need to **tokenize** the text, remove stopwords, or **lemmatize** the text. What you do in pre-processing depends entirely on what your project is.\n",
    "\n",
    "Check out [spaCy 101](https://spacy.io/usage/spacy-101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "pizh0wfAQkAf"
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18132\\3909579629.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"en_core_web_sm\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\__init__.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[0mRETURNS\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mLanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mloaded\u001b[0m \u001b[0mnlp\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \"\"\"\n\u001b[1;32m---> 54\u001b[1;33m     return util.load_model(\n\u001b[0m\u001b[0;32m     55\u001b[0m         \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[0mvocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\util.py\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m    437\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mOLD_MODEL_SHORTCUTS\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    438\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE941\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfull\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mOLD_MODEL_SHORTCUTS\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[index]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 439\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    440\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = \"IN the second century of the Christian era, the Empire of Rome comprehended the fairest part of the earth, and the most civilised portion of mankind.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nlp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18132\\3963823117.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'nlp' is not defined"
     ]
    }
   ],
   "source": [
    "doc = nlp(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'doc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18132\\2340623638.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlemma_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_alpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_stop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'doc' is not defined"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory issues with spaCy\n",
    "We will be using spAcy to process Gibbon's chapters, but some of them can be quite long (over 3 million characters). This means that spAcy can run into memory problems. As a solution, I will disable a few of spaCy's functions and raise its default allowed lengh. For an alternative solution, see below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find longest chapter so I know what value to use for nlp.max_length\n",
    "text_path = \"../text/gibbon_decline_and_fall/\"\n",
    "longest = 0\n",
    "for file_name in os.listdir(text_path):\n",
    "    with open(text_path + file_name, encoding='utf-8', mode='r') as f:\n",
    "        raw_text = f.read()\n",
    "    text_len = len(raw_text)\n",
    "    if text_len > longest:\n",
    "        longest = text_len\n",
    "print(longest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.disable_pipes('ner', 'parser')\n",
    "nlp.max_length = 3045039"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process Gibbon\n",
    "\n",
    "For our immediate purposes we want to convert the raw text of Gibbon (which is in the form of `strings`) to a list of **lemmas**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_noun_and_verb_lemmas(text):\n",
    "    \"\"\"Return a list of noun and verb lemmas from a string\"\"\"\n",
    "    doc = nlp(text)\n",
    "    tokens = [token for token in doc]\n",
    "    noun_and_verb_tokens = [token for token in tokens if token.pos_ == 'NOUN' or token.pos_ == 'VERB']\n",
    "    noun_and_verb_lemmas = [noun_and_verb_token.lemma_ for noun_and_verb_token in noun_and_verb_tokens]\n",
    "    return noun_and_verb_lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes about 3 mintues\n",
    "text_path = \"../text/gibbon_decline_and_fall/\"\n",
    "gibbon_lemmas = {}\n",
    "for file_name in os.listdir(text_path):\n",
    "    chapter_name = file_name[23:29]\n",
    "    with open(text_path + file_name, encoding='utf-8', mode = 'r') as f:\n",
    "        raw_text = f.read()\n",
    "    lemmas = get_noun_and_verb_lemmas(raw_text)\n",
    "    gibbon_lemmas[chapter_name] = lemmas\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I752weTCKR-l",
    "outputId": "ebff87dd-32b7-4882-d379-27b290d47647"
   },
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "print(len(gibbon_lemmas))\n",
    "print(gibbon_lemmas.keys())\n",
    "print(gibbon_lemmas['chap01'][:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../data/'\n",
    "file_name = 'gibbon_lemmas.json'\n",
    "with open(file_path + file_name, encoding='utf-8', mode='w') as f:\n",
    "    json.dump(gibbon_lemmas, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative solution to the memory issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempt 2: \n",
    "text_path = \"../text/gibbon_decline_and_fall/\"\n",
    "gibbon_lemmas = {}\n",
    "for file_name in os.listdir(text_path):\n",
    "    chapter_name = file_name[23:29]\n",
    "    with open(text_path + file_name, encoding='utf-8', mode = 'r') as f:\n",
    "        raw_text = f.read()\n",
    "    if len(raw_text) < 1000000:  # SpaCy will throw a memory error if a text is more than 1,000,000 characters\n",
    "        lemmas = get_noun_and_verb_lemmas(raw_text)\n",
    "        gibbon_lemmas[chapter_name] = lemmas\n",
    "    else:\n",
    "        print(f\"Long chapter: {chapter_name}\")\n",
    "        lemmas = []\n",
    "        text_lines = raw_text.split('\\n')\n",
    "        for text_line in text_lines:\n",
    "            line_lemmas = get_noun_and_verb_lemmas(text_line)\n",
    "            for line_lemma in line_lemmas:\n",
    "                lemmas.append(line_lemma)\n",
    "        gibbon_lemmas[chapter_name] = lemmas"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "explore_gibbon.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
