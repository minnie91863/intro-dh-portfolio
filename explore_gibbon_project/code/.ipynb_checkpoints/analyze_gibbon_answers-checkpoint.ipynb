{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hApv2hxy_KO0"
   },
   "source": [
    "# Exploring Gibbon - Analyze Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z4bCFyUaJ1cA"
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import json\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "file_path = '../data/'\n",
    "file_name = 'gibbon_lemmas.json'\n",
    "with open(file_path + file_name, encoding='utf-8', mode='r') as f:\n",
    "    gibbon_lemmas = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a-i5QjIaJfd-"
   },
   "source": [
    "## Find the most important words by chapter in Gibbon\n",
    "For this part we are going to use a library called [scikit-learn](https://scikit-learn.org/stable/). This library is primarily for machine learning, but many of its features are useful for DH work.\n",
    "Advanced Reading: https://towardsdatascience.com/tf-idf-explained-and-python-sklearn-implementation-b020c5e83275"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VTGExuH9YyOv"
   },
   "outputs": [],
   "source": [
    "# The tool I will use here requires a string as input rather than a list, so I convert my docs from lists to strings\n",
    "gibbon_chap_strings = []\n",
    "gibbon_chap_names = []\n",
    "for key, value in gibbon_lemmas.items():\n",
    "    gibbon_chap_names.append(key)  \n",
    "    chap_string = ' '.join(value)\n",
    "    gibbon_chap_strings.append(chap_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jjctTms2WWar"
   },
   "outputs": [],
   "source": [
    "# transform corpus into a matrix of word counts\n",
    "vectorizer = TfidfVectorizer(max_df=.65, min_df=1, stop_words=None, \n",
    "                             use_idf= True, norm=None)\n",
    "transformed_chaps = vectorizer.fit_transform(gibbon_chap_strings)\n",
    "transformed_chaps_as_array = transformed_chaps.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VFR-PdNBWyhA"
   },
   "outputs": [],
   "source": [
    "gibbon_key_vocab_by_chap = {}\n",
    "for chap, chap_name in zip(transformed_chaps_as_array, gibbon_chap_names):\n",
    "    tf_idf_tuples = list(zip(vectorizer.get_feature_names(), chap))\n",
    "    sorted_tf_idf_tuples = sorted(tf_idf_tuples, key= lambda x: x[1], reverse=True)\n",
    "    k = chap_name\n",
    "    v = sorted_tf_idf_tuples[:10]  # only getting the top ten\n",
    "    gibbon_key_vocab_by_chap[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lOtAg3yrN2n6",
    "outputId": "e8f042ad-2d51-4736-e23d-6b8f6a218c35"
   },
   "outputs": [],
   "source": [
    "for k, v in gibbon_key_vocab_by_chap.items():\n",
    "    result = k + ' => ' + v[0][0] + ', ' + v[1][0] + ', ' + v[2][0] + ', ' + v[3][0] + ', ' + v[4][0]\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ujc-mmSMZohr",
    "outputId": "56b3350d-d692-471b-cb0e-88ca45b6c45e"
   },
   "outputs": [],
   "source": [
    "# explore vocabulary\n",
    "gibbon_key_vocab_by_chap['chap01']  # <-- you can investigate other chapters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0thCE3jvaVqp"
   },
   "source": [
    "## Conditional frequency distribution in Gibbon\n",
    "\n",
    "### Natural Language Toolkit\n",
    "The **Natural Language Toolkit** (NLTK) is a library used for natural language processing (NLP). If you want to learn more, I highly recommend working through the [NLTK Book](https://www.nltk.org/book/). This resource is a great introduction to NLP specifically and Python more generally.\n",
    "\n",
    "A **conditional frequency distribution** (cfd) is a collection of word counts for a given condition, i.e. category. Here the category is separate chapters in Gibbon. We can chart what used are used most frequently by chapter. This will tell us something about the nature of each chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cp-vlGgoMaFL",
    "outputId": "abf6549d-22ef-49bc-c588-c6e5dcc9d532"
   },
   "outputs": [],
   "source": [
    "# conditional frequency distribution\n",
    "# Note: I adapted these lines of code from the NLTK\n",
    "key_words = ['doctrine', 'apostle', 'presbyter', 'daemon', 'immortality']  # <-- instert token(s) to explore (lowercase)\n",
    "cfd = nltk.ConditionalFreqDist(\n",
    "    (key_word, chap_name)\n",
    "    for chap_name in gibbon_lemmas.keys()\n",
    "    for lemma in gibbon_lemmas[chap_name]\n",
    "    for key_word in key_words\n",
    "    if lemma.lower().startswith(key_word)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display plot\n",
    "plt.figure(figsize=(20, 8))  # this expands the plot to make it more readable\n",
    "cfd.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ih3TwGldGhEG"
   },
   "source": [
    "### Activity\n",
    "Based on the key vocabulary by chapter above, explore the use of different terms in the conditional frequency distribution. \n",
    "* What questions about the text does this raise for you?\n",
    "* What hypotheses about the text can you form?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "explore_gibbon.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
