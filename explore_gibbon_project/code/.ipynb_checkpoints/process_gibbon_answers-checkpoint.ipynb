{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hApv2hxy_KO0"
   },
   "source": [
    "# Exploring Gibbon - Process Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Z4bCFyUaJ1cA"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18132\\3888832993.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import os\n",
    "import json\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary: Working with files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open a file and write to it\n",
    "text = \"In the second century of the Christian era, the empire of Rome comprehended the fairest part of the earth, and the most civilised portion of mankind. The frontiers of that extensive monarchy were guarded by ancient renown and disciplined valour. The gentle, but powerful, influence of laws and manners had gradually cemented the union of the provinces.\"\n",
    "file_name = 'new_file.txt'\n",
    "with open(file=file_name, encoding='utf-8', mode='w') as f:\n",
    "    f.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open a file to read from it\n",
    "with open(file=file_name, encoding='utf-8', mode ='r') as f:\n",
    "    opened_text = f.read()\n",
    "print(opened_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary: List Comprehension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember our first function for NLP?\n",
    "text = \"In the second century of the Christian era, the empire of Rome comprehended the fairest part of the earth, and the most civilised portion of mankind. The frontiers of that extensive monarchy were guarded by ancient renown and disciplined valour. The gentle, but powerful, influence of laws and manners had gradually cemented the union of the provinces.\"\n",
    "# Convert the string into a list\n",
    "tokens = text.split(\" \")\n",
    "# Identify stop-words\n",
    "stop_words = [\"in\", \"the\", \"of\", \"and\", \"that\", \"by\", \"but\"]\n",
    "# Create an empty list for tokens that aren't stop words\n",
    "tokens_no_stops = []\n",
    "# Iterate through tokens\n",
    "for token in tokens:\n",
    "    if token.lower() not in stop_words:\n",
    "        tokens_no_stops.append(token)\n",
    "print(tokens_no_stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The for loop can be re-written as a \"list comprehension\"\n",
    "text_2 = \"In the second century of the Christian era, the empire of Rome comprehended the fairest part of the earth, and the most civilised portion of mankind. The frontiers of that extensive monarchy were guarded by ancient renown and disciplined valour. The gentle, but powerful, influence of laws and manners had gradually cemented the union of the provinces.\"\n",
    "# Convert the string into a list\n",
    "tokens_2 = text_2.split(\" \")\n",
    "# Identify stop-words\n",
    "stop_words = [\"in\", \"the\", \"of\", \"and\", \"that\", \"by\", \"but\"]\n",
    "# List comprehension\n",
    "tokens_no_stops_2 = [token for token in tokens if not token.lower() in stop_words]\n",
    "print(tokens_no_stops_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary: os.listdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use os.listdir to list file names in a directory\n",
    "text_path = \"../text/gibbon_decline_and_fall/\"\n",
    "for file_name in os.listdir(text_path):\n",
    "    print(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KG63QU6DIhgZ"
   },
   "source": [
    "## Process text for analysis using spaCy\n",
    "\n",
    "Before doing NLP work, most texts will need to be preprocessed in different ways. You may need to **tokenize** the text, remove stopwords, or **lemmatize** the text. What you do in pre-processing depends entirely on what your project is.\n",
    "\n",
    "Check out [spaCy 101](https://spacy.io/usage/spacy-101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pizh0wfAQkAf"
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = \"IN the second century of the Christian era, the Empire of Rome comprehended the fairest part of the earth, and the most civilised portion of mankind.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory issues with spaCy\n",
    "We will be using spAcy to process Gibbon's chapters, but some of them can be quite long (over 3 million characters). This means that spAcy can run into memory problems. As a solution, I will disable a few of spaCy's functions and raise its default allowed lengh. For an alternative solution, see below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find longest chapter so I know what value to use for nlp.max_length\n",
    "text_path = \"../text/gibbon_decline_and_fall/\"\n",
    "longest = 0\n",
    "for file_name in os.listdir(text_path):\n",
    "    with open(text_path + file_name, encoding='utf-8', mode='r') as f:\n",
    "        raw_text = f.read()\n",
    "    text_len = len(raw_text)\n",
    "    if text_len > longest:\n",
    "        longest = text_len\n",
    "print(longest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.disable_pipes('ner', 'parser')\n",
    "nlp.max_length = 3045039"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process Gibbon\n",
    "\n",
    "For our immediate purposes we want to convert the raw text of Gibbon (which is in the form of `strings`) to a list of **lemmas**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_noun_and_verb_lemmas(text):\n",
    "    \"\"\"Return a list of noun and verb lemmas from a string\"\"\"\n",
    "    doc = nlp(text)\n",
    "    tokens = [token for token in doc]\n",
    "    noun_and_verb_tokens = [token for token in tokens if token.pos_ == 'NOUN' or token.pos_ == 'VERB']\n",
    "    noun_and_verb_lemmas = [noun_and_verb_token.lemma_ for noun_and_verb_token in noun_and_verb_tokens]\n",
    "    return noun_and_verb_lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes about 3 mintues\n",
    "text_path = \"../text/gibbon_decline_and_fall/\"\n",
    "gibbon_lemmas = {}\n",
    "for file_name in os.listdir(text_path):\n",
    "    chapter_name = file_name[23:29]\n",
    "    with open(text_path + file_name, encoding='utf-8', mode = 'r') as f:\n",
    "        raw_text = f.read()\n",
    "    lemmas = get_noun_and_verb_lemmas(raw_text)\n",
    "    gibbon_lemmas[chapter_name] = lemmas\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I752weTCKR-l",
    "outputId": "ebff87dd-32b7-4882-d379-27b290d47647"
   },
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "print(len(gibbon_lemmas))\n",
    "print(gibbon_lemmas.keys())\n",
    "print(gibbon_lemmas['chap01'][:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../data/'\n",
    "file_name = 'gibbon_lemmas.json'\n",
    "with open(file_path + file_name, encoding='utf-8', mode='w') as f:\n",
    "    json.dump(gibbon_lemmas, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative solution to the memory issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempt 2: \n",
    "text_path = \"../text/gibbon_decline_and_fall/\"\n",
    "gibbon_lemmas = {}\n",
    "for file_name in os.listdir(text_path):\n",
    "    chapter_name = file_name[23:29]\n",
    "    with open(text_path + file_name, encoding='utf-8', mode = 'r') as f:\n",
    "        raw_text = f.read()\n",
    "    if len(raw_text) < 1000000:  # SpaCy will throw a memory error if a text is more than 1,000,000 characters\n",
    "        lemmas = get_noun_and_verb_lemmas(raw_text)\n",
    "        gibbon_lemmas[chapter_name] = lemmas\n",
    "    else:\n",
    "        print(f\"Long chapter: {chapter_name}\")\n",
    "        lemmas = []\n",
    "        text_lines = raw_text.split('\\n')\n",
    "        for text_line in text_lines:\n",
    "            line_lemmas = get_noun_and_verb_lemmas(text_line)\n",
    "            for line_lemma in line_lemmas:\n",
    "                lemmas.append(line_lemma)\n",
    "        gibbon_lemmas[chapter_name] = lemmas"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "explore_gibbon.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
